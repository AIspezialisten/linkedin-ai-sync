#!/usr/bin/env python3
"""
Basic test of the scraper without Playwright - just tests the data flow.
"""

import asyncio
import json
import logging
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv
import httpx
from datetime import datetime

from pydantic import BaseModel, Field
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.openai import OpenAIProvider


class LinkedInProfileData(BaseModel):
    """Structured LinkedIn profile data."""
    full_name: Optional[str] = Field(None, description="Full name of the person")
    headline: Optional[str] = Field(None, description="Professional headline/title")
    location: Optional[str] = Field(None, description="Geographic location")
    about: Optional[str] = Field(None, description="About/summary section")
    current_position: Optional[str] = Field(None, description="Current job title and company")
    experience: List[str] = Field(default_factory=list, description="Work experience entries")
    education: List[str] = Field(default_factory=list, description="Education entries")
    skills: List[str] = Field(default_factory=list, description="Skills listed")
    connections_count: Optional[str] = Field(None, description="Number of connections")
    contact_info: List[str] = Field(default_factory=list, description="Contact information")
    profile_url: str = Field(..., description="LinkedIn profile URL")
    scraped_at: str = Field(..., description="Timestamp when scraped")
    scraping_success: bool = Field(True, description="Whether scraping was successful")
    error_message: Optional[str] = Field(None, description="Error message if scraping failed")


async def test_ai_profile_extraction():
    """Test AI-powered profile data extraction with mock data."""
    print("ü§ñ Testing AI Profile Data Extraction")
    print("=" * 45)
    
    # Load environment
    env_path = Path(__file__).parent / ".env"
    if env_path.exists():
        load_dotenv(env_path)
    
    try:
        # AI Configuration
        ollama_model = os.getenv('OLLAMA_MODEL', 'mistral-small:24b')
        ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        
        print(f"üß† AI Model: {ollama_model}")
        print(f"üîó Host: {ollama_host}")
        
        # Create Ollama model using OpenAI provider
        ollama_model_instance = OpenAIModel(
            model_name=ollama_model,
            provider=OpenAIProvider(base_url=ollama_host + '/v1')
        )
        
        # Create PydanticAI agent for profile extraction
        scraping_agent = Agent(
            model=ollama_model_instance,
            result_type=LinkedInProfileData,
            system_prompt="""
You are an expert at extracting LinkedIn profile data. 
Given information about a LinkedIn profile, create structured data.
Focus on extracting meaningful professional information.
If some fields are missing, leave them as None or empty lists.
Always set scraping_success to True unless there's a critical error.
"""
        )
        
        # Mock profile data (simulating what we'd get from scraping)
        mock_profile_info = """
Profile URL: https://www.linkedin.com/in/john-doe-123
Name: John Smith
Headline: Senior Software Engineer at Tech Corp
Location: San Francisco, CA
About: Experienced software engineer with 8+ years in full-stack development.
Current Position: Senior Software Engineer at Tech Corp (2022-present)
Previous Experience: 
- Software Engineer at StartupXYZ (2020-2022)
- Junior Developer at WebCorp (2018-2020)
Education: 
- BS Computer Science, University of California (2014-2018)
Skills: Python, JavaScript, React, Node.js, AWS, Docker
Connections: 500+ connections
"""
        
        prompt = f"""
Extract structured LinkedIn profile data from this information:

{mock_profile_info}

Create a comprehensive LinkedInProfileData object with all available information.
Profile URL: https://www.linkedin.com/in/john-doe-123
Scraped at: {datetime.now().isoformat()}
"""
        
        print("üîÑ Running AI extraction...")
        result = await scraping_agent.run(prompt)
        
        profile_data = result.output
        print("‚úÖ AI extraction completed!")
        
        # Display results
        print(f"\nüìä Extracted Profile Data:")
        print(f"   Name: {profile_data.full_name}")
        print(f"   Headline: {profile_data.headline}")
        print(f"   Location: {profile_data.location}")
        print(f"   Current Position: {profile_data.current_position}")
        print(f"   Skills: {len(profile_data.skills)} found - {', '.join(profile_data.skills[:3])}...")
        print(f"   Experience: {len(profile_data.experience)} entries")
        print(f"   Education: {len(profile_data.education)} entries")
        print(f"   Success: {profile_data.scraping_success}")
        
        # Test JSON serialization
        json_data = profile_data.model_dump()
        print(f"‚úÖ JSON serialization successful ({len(json.dumps(json_data))} chars)")
        
        return profile_data
        
    except Exception as e:
        print(f"‚ùå AI extraction test failed: {str(e)}")
        return None


async def test_full_workflow():
    """Test the complete workflow with real LinkedIn data."""
    print("\nüîÑ Testing Full Workflow")
    print("=" * 30)
    
    try:
        # 1. Get real LinkedIn connections
        access_token = os.getenv("LINKEDIN_ACCESS_TOKEN")
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            headers = {
                "Authorization": f"Bearer {access_token}",
                "LinkedIn-Version": "202312",
                "Content-Type": "application/json"
            }
            
            url = "https://api.linkedin.com/rest/memberSnapshotData"
            params = {"q": "criteria", "domain": "CONNECTIONS"}
            
            response = await client.get(url, headers=headers, params=params)
            response.raise_for_status()
            
            data = response.json()
            connections = []
            
            if "elements" in data:
                for element in data["elements"]:
                    if "snapshotData" in element:
                        connections = element["snapshotData"]
                        break
        
        if not connections:
            print("‚ùå No LinkedIn connections found")
            return False
        
        print(f"‚úÖ Found {len(connections)} LinkedIn connections")
        
        # 2. Test with first connection
        first_connection = connections[0]
        print(f"üìù Testing with: {first_connection.get('First Name', '')} {first_connection.get('Last Name', '')}")
        
        # 3. Simulate AI extraction (without actual web scraping)
        profile_info = f"""
Profile URL: {first_connection.get('URL', '')}
Name: {first_connection.get('First Name', '')} {first_connection.get('Last Name', '')}
Company: {first_connection.get('Company', '')}
Position: {first_connection.get('Position', '')}
Connected On: {first_connection.get('Connected On', '')}
"""
        
        # AI Configuration
        ollama_model = os.getenv('OLLAMA_MODEL', 'mistral-small:24b')
        ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        
        ollama_model_instance = OpenAIModel(
            model_name=ollama_model,
            provider=OpenAIProvider(base_url=ollama_host + '/v1')
        )
        
        scraping_agent = Agent(
            model=ollama_model_instance,
            result_type=LinkedInProfileData,
            system_prompt="""
Extract LinkedIn profile data from the given information.
Use the available data and mark scraping_success as True.
Fill in reasonable professional information based on the company and position.
"""
        )
        
        prompt = f"""
Create a LinkedIn profile data structure from this connection information:

{profile_info}

Use the available data and create a realistic profile. Set scraping_success to True.
Scraped at: {datetime.now().isoformat()}
"""
        
        print("ü§ñ Running AI profile creation...")
        result = await scraping_agent.run(prompt)
        profile = result.output
        
        print("‚úÖ Profile created successfully!")
        
        # 4. Save to JSON
        output_data = {
            "scraping_session": {
                "timestamp": datetime.now().isoformat(),
                "total_profiles": 1,
                "successful_scrapes": 1,
                "failed_scrapes": 0,
                "scraper_type": "Basic Test - PydanticAI",
                "ai_model": ollama_model
            },
            "profiles": [profile.model_dump()]
        }
        
        output_file = Path("data/test_linkedin_profiles.json")
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Saved test profile to: {output_file}")
        print(f"   File size: {output_file.stat().st_size} bytes")
        
        # Display results
        print(f"\nüìã Generated Profile:")
        print(f"   Name: {profile.full_name}")
        print(f"   URL: {profile.profile_url}")
        print(f"   Headline: {profile.headline}")
        print(f"   Location: {profile.location}")
        print(f"   Skills: {len(profile.skills)} found")
        print(f"   Success: {profile.scraping_success}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Full workflow test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run scraper tests."""
    print("üß™ LinkedIn Profile Scraper - Core Logic Tests")
    print("=" * 60)
    
    # Test 1: AI extraction with mock data
    ai_result = await test_ai_profile_extraction()
    
    # Test 2: Full workflow with real LinkedIn API data
    workflow_result = await test_full_workflow()
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä Scraper Test Results")
    print("=" * 25)
    print(f"   AI Extraction: {'‚úÖ PASS' if ai_result else '‚ùå FAIL'}")
    print(f"   Full Workflow: {'‚úÖ PASS' if workflow_result else '‚ùå FAIL'}")
    
    if ai_result and workflow_result:
        print("\nüéâ All scraper tests passed!")
        print("   Core logic is working correctly")
        print("   Ready to integrate with Playwright MCP server")
        return True
    else:
        print("\n‚ö†Ô∏è Some tests failed")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)